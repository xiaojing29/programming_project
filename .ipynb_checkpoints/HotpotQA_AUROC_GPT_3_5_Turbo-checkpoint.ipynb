{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "889caca0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "889caca0",
    "outputId": "98dd733f-43d3-4219-8073-699ee4c17e3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==0.28.0 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from openai==0.28.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from openai==0.28.0) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from openai==0.28.0) (3.8.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.0) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28.0) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28.0) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28.0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28.0) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28.0) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28.0) (1.2.0)\n",
      "Requirement already satisfied: peft in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (0.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from peft) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from peft) (23.0)\n",
      "Requirement already satisfied: psutil in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from peft) (6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from peft) (2.2.2)\n",
      "Requirement already satisfied: transformers in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from peft) (4.39.3)\n",
      "Requirement already satisfied: tqdm in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from peft) (4.65.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from peft) (0.24.1)\n",
      "Requirement already satisfied: safetensors in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from peft) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from peft) (0.20.3)\n",
      "Requirement already satisfied: filelock in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2023.12.2)\n",
      "Requirement already satisfied: requests in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from transformers->peft) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from transformers->peft) (0.15.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: transformers in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (4.39.3)\n",
      "Requirement already satisfied: filelock in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: torch in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==0.28.0\n",
    "!pip install peft\n",
    "!pip install transformers\n",
    "!pip install numpy\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "-nEuM2XFHdSn",
   "metadata": {
    "id": "-nEuM2XFHdSn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key:········\n",
      "env: OPENAI_API_KEY=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\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "openai_api_key = getpass.getpass('Enter your OpenAI API key:')\n",
    "%env OPENAI_API_KEY=$openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "EqqvsCOjWxyk",
   "metadata": {
    "id": "EqqvsCOjWxyk"
   },
   "outputs": [],
   "source": [
    "# LLMs\n",
    "import openai\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Initialize OpenAI API\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "def call_openai_model(prompt, model, temperature):\n",
    "    response = None\n",
    "    while response is None:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature = temperature\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            if 'is greater than the maximum' in str(e):\n",
    "                raise BatchSizeException()\n",
    "            print(e)\n",
    "            print('Retrying...')\n",
    "            time.sleep(2)\n",
    "        try:\n",
    "            output = response.choices[0].message.content\n",
    "        except Exception:\n",
    "            output = 'do not have reponse from chatgpt'\n",
    "    return output\n",
    "\n",
    "\n",
    "def call_guanaco_33b(prompt, max_new_tokens):\n",
    "    # 16 float\n",
    "    model_name = \"huggyllama/llama-30b\"\n",
    "    adapters_name = 'timdettmers/guanaco-33b'\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        # offload_folder=\"/home/ec2-user/SageMaker/hf_cache\",\n",
    "        max_memory= {i: '16384MB' for i in range(torch.cuda.device_count())}, # V100 16GB\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(model, adapters_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # prompt\n",
    "    formatted_prompt = (\n",
    "        f\"A chat between a curious human and an artificial intelligence assistant.\"\n",
    "        f\"The assistant gives helpful, concise, and polite answers to the user's questions.\\n\"\n",
    "        f\"### Human: {prompt} ### Assistant:\"\n",
    "    )\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    outputs = model.generate(inputs=inputs.input_ids, max_new_tokens=max_new_tokens)\n",
    "    res = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    res_sp = res.split('###')\n",
    "    output = res_sp[1] + res_sp[2]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def call_falcon_7b(prompt, max_new_tokens):\n",
    "    # 16 float\n",
    "    model = \"tiiuae/falcon-7b-instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    sequences = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    for seq in sequences:\n",
    "        res = seq['generated_text']\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "R1u4IOGjYD7o",
   "metadata": {
    "id": "R1u4IOGjYD7o"
   },
   "outputs": [],
   "source": [
    "# paraphraser\n",
    "def paraphrase(question, number, model, temperature):\n",
    "    '''\n",
    "    Inputs:\n",
    "    quesiton - original user query\n",
    "    number - how many perturbed questions\n",
    "    model - GPTs or open-sourced models\n",
    "    temperature - typically we use 0 here\n",
    "\n",
    "    Output:\n",
    "    perb_questions - perturbed questions that are semantically equivalent to the question\n",
    "    '''\n",
    "\n",
    "    perb_questions = []\n",
    "    prompt_temp = f'For question Q, provide {number} semantically equivalent questions.'\n",
    "    prompt = prompt_temp + '\\nQ:' + question\n",
    "\n",
    "    res = call_openai_model(prompt, model, temperature) # openai model call\n",
    "    res_split = res.split('\\n')\n",
    "    for i in range(len(res_split)):\n",
    "        perb_questions.append(res_split[i])\n",
    "\n",
    "    return perb_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6EzG0rWbLmXd",
   "metadata": {
    "id": "6EzG0rWbLmXd"
   },
   "outputs": [],
   "source": [
    "# Consistency_Checker\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "\n",
    "class SemanticConsistnecyCheck:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.prompt_temp = \"\"\"\n",
    "        Are the following two Question-Answer(QA) pairs semantically equivalent?\n",
    "        Provide your best guess and the probability that it is correct (0.0 to 1.0).\n",
    "        Given ONLY the guess (Yes or No) and probability, no other words or explanation.\n",
    "        For example:\n",
    "        Guess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
    "        Probability: <the probability between 0.0 and 1.0 that your guess is correct, without any extra commentary whatsoever;\n",
    "        just the probability!>\n",
    "        \"\"\"\n",
    "\n",
    "    def openai_api_parallel(self, prompt, temperature):\n",
    "        res = call_openai_model(prompt, self.model, temperature) # openai model call\n",
    "        return res\n",
    "\n",
    "    def score_scc_api(self, question, target_answer, candidate_answers, temperature):\n",
    "\n",
    "        if target_answer is None:\n",
    "            raise ValueError(\"Target answer cannot be None. \")\n",
    "\n",
    "        sc_output = []\n",
    "        target_pair = 'Q:' + question + '\\nA:' + target_answer\n",
    "        num_candidate_answer = len(candidate_answers)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=num_candidate_answer+2) as executor:\n",
    "            all_res = []\n",
    "            for i in range(num_candidate_answer):\n",
    "                candidate_pair = 'Q:' + question + '\\nA:' + candidate_answers[i]\n",
    "                prompt = self.prompt_temp + '\\nThe first QA pair is:\\n' + target_pair + '\\nThe second QA pair is:\\n' + candidate_pair\n",
    "                output = executor.submit(self.openai_api_parallel, prompt, temperature)\n",
    "                all_res.append(output)\n",
    "\n",
    "            for temp in concurrent.futures.as_completed(all_res):\n",
    "                res = temp.result()\n",
    "                guess = res.split(':')[1].split('\\n')[0].strip()\n",
    "                # print(res, guess)\n",
    "                value = 0 if guess == 'Yes' else 1\n",
    "                # print('value',value)\n",
    "                sc_output.append(value)\n",
    "\n",
    "        score = sum(sc_output)/num_candidate_answer\n",
    "        return score, sc_output\n",
    "\n",
    "\n",
    "\n",
    "    def score_scc(self, question, target_answer, candidate_answers, temperature):\n",
    "        '''\n",
    "        Inputs:\n",
    "        question - original user query\n",
    "        target_answer - generated response given the original question (temp=0) if not provided by user\n",
    "        candidate_answers - generated responses given the question (original + perturbed)\n",
    "        temperature - [0,1] for LLM randomness\n",
    "\n",
    "        Outputs:\n",
    "        score - inconsistency score (hallucination metric)\n",
    "        sc_output - specific score for each candidate answers compared with the target answer\n",
    "        '''\n",
    "\n",
    "        if target_answer is None:\n",
    "            raise ValueError(\"Target answer cannot be None. \")\n",
    "\n",
    "        sc_output = []\n",
    "        target_pair = 'Q:' + question + '\\nA:' + target_answer\n",
    "        num_candidate_answer = len(candidate_answers)\n",
    "        for i in range(num_candidate_answer):\n",
    "            candidate_pair = 'Q:' + question + '\\nA:' + candidate_answers[i]\n",
    "            prompt = self.prompt_temp + '\\nThe first QA pair is:\\n' + target_pair + '\\nThe second QA pair is:\\n' + candidate_pair\n",
    "            res = call_openai_model(prompt, self.model, temperature) # openai model call\n",
    "            # res = call_guanaco_33b(prompt, max_new_tokens=200)  # guanaco_33b model call\n",
    "            # res = call_falcon_7b(prompt, max_new_tokens = 200) # falcon_7b model call\n",
    "            guess = res.split(':')[1].split('\\n')[0].strip()\n",
    "            # print(res, guess)\n",
    "            value = 0 if guess == 'Yes' else 1\n",
    "            # print('value',value)\n",
    "            sc_output.append(value)\n",
    "\n",
    "        score = sum(sc_output)/num_candidate_answer\n",
    "        return score, sc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hNM8NNHgYa-U",
   "metadata": {
    "id": "hNM8NNHgYa-U"
   },
   "outputs": [],
   "source": [
    "# evaluator\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "\n",
    "class Evaluate:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.prompt_temp = 'Please answer the following question:\\n'\n",
    "\n",
    "    def openai_api_parallel(self, prompt, temperature):\n",
    "        res = call_openai_model(prompt, self.model, temperature) # openai model call\n",
    "        return res\n",
    "\n",
    "#     def self_evaluate_api(self, self_question, temperature, self_num):\n",
    "\n",
    "#         prompt = self.prompt_temp + '\\nQ:' + self_question\n",
    "#         self_responses = []\n",
    "#         with ThreadPoolExecutor(max_workers=self_num) as executor:\n",
    "#             outputs = executor.map(self.openai_api_parallel, prompt, temperature)\n",
    "#             for res in outputs:\n",
    "#                 self_responses.append(res)\n",
    "\n",
    "#         return self_responses\n",
    "\n",
    "    def self_evaluate_api(self, self_question, temperature, self_num):\n",
    "\n",
    "        prompt = self.prompt_temp + self_question\n",
    "        self_responses = []\n",
    "        with ThreadPoolExecutor(max_workers=self_num) as executor:\n",
    "            futures = [executor.submit(self.openai_api_parallel, prompt, temperature) for _ in range(self_num)]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                self_responses.append(future.result())\n",
    "\n",
    "        return self_responses\n",
    "\n",
    "\n",
    "\n",
    "    def self_evaluate(self, self_question, temperature, self_num):\n",
    "        '''\n",
    "        Inputs:\n",
    "        self_question - original user query\n",
    "        temperature - [0,1] for LLM randomness\n",
    "        self_num - how many generated responses given this question\n",
    "\n",
    "        Outputs:\n",
    "        self_responses - generated responses given this question with different temperatures\n",
    "        '''\n",
    "\n",
    "        self_responses = []\n",
    "        prompt = self.prompt_temp + '\\nQ:' + self_question\n",
    "\n",
    "        for i in range(self_num):\n",
    "            # llm model: GPTs, open-source models (falcon, guanaco)\n",
    "            if self.model in ['gpt-3.5-turbo','gpt-4']:\n",
    "                res = call_openai_model(prompt, self.model, temperature) # openai model call\n",
    "            elif self.model == 'guanaco-33b':\n",
    "                res = call_guanaco_33b(prompt, max_new_tokens = 200)\n",
    "            elif self.model == 'falcon-7b':\n",
    "                res = call_falcon_7b(prompt, max_new_tokens = 200)\n",
    "            # other open-sourced llms\n",
    "            self_responses.append(res)\n",
    "\n",
    "        # thread\n",
    "\n",
    "        return self_responses\n",
    "\n",
    "\n",
    "    def perb_evaluate_api(self, perb_questions, temperature):\n",
    "        '''\n",
    "        Inputs:\n",
    "        perb_questions - perturbed questions that are semantically equivalent to the original question\n",
    "        temperature - [0,1] for LLM randomness\n",
    "\n",
    "        Outputs:\n",
    "        perb_responses - generated responses given the perturbed questions\n",
    "        '''\n",
    "\n",
    "        perb_responses = []\n",
    "        with ThreadPoolExecutor(max_workers=len(perb_questions)) as executor:\n",
    "            future_to_pq = {\n",
    "                executor.submit(self.openai_api_parallel, self.prompt_temp + perb_question, temperature): perb_question\n",
    "                for perb_question in perb_questions\n",
    "            }\n",
    "\n",
    "            for future in concurrent.futures.as_completed(future_to_pq):\n",
    "                perb_question = future_to_pq[future]\n",
    "                try:\n",
    "                    perb_responses.append(future.result())\n",
    "                except Exception as exc:\n",
    "                    print('%r generated an exception: %s' % (perb_question, exc))\n",
    "\n",
    "        return perb_responses\n",
    "\n",
    "\n",
    "\n",
    "    def perb_evaluate(self, perb_questions, temperature):\n",
    "        '''\n",
    "        Inputs:\n",
    "        perb_questions - perturbed questions that are semantically equivalent to the original question\n",
    "        temperature - [0,1] for LLM randomness\n",
    "\n",
    "        Outputs:\n",
    "        perb_responses - generated responses given the perturbed questions\n",
    "        '''\n",
    "\n",
    "        perb_responses = []\n",
    "        for i in range(len(perb_questions)):\n",
    "            prompt = self.prompt_temp + '\\nQ:' + perb_questions[i]\n",
    "            # llm model: GPTs, open-source models (falcon, guanaco)\n",
    "            if self.model in ['gpt-3.5-turbo','gpt-4']:\n",
    "                res = call_openai_model(prompt, self.model, temperature) # openai model call\n",
    "            elif self.model == 'guanaco-33b':\n",
    "                res = call_guanaco_33b(prompt, max_new_tokens = 200)\n",
    "            elif self.model == 'falcon-7b':\n",
    "                res = call_falcon_7b(prompt, max_new_tokens = 200)\n",
    "            # other open-sourced llms\n",
    "            perb_responses.append(res)\n",
    "\n",
    "        return perb_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "PPnpjcoXoirr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "PPnpjcoXoirr",
    "outputId": "259b0d3e-88e7-4686-857b-8b6f9201e06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'knowledge': \"Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.First for Women is a woman's magazine published by Bauer Media Group in the USA.\", 'question': \"Which magazine was started first Arthur's Magazine or First for Women?\", 'right_answer': \"Arthur's Magazine\", 'hallucinated_answer': 'First for Women was started first.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_dataset(filepath):\n",
    "    \"\"\"Loads the dataset from a specified file path.\n",
    "\n",
    "    Args:\n",
    "    filepath (str): The path to the dataset file.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of JSON objects loaded from the file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append(json_obj)\n",
    "    return data\n",
    "\n",
    "# Adjust the path below if your dataset file is in a different directory\n",
    "dataset_path = 'hotpotQA_halu.json'\n",
    "qa_data = load_dataset(dataset_path)\n",
    "\n",
    "# Example usage:\n",
    "print(qa_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lGhbUuZzpHx5",
   "metadata": {
    "id": "lGhbUuZzpHx5"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_dataset(filepath):\n",
    "    data = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append(json_obj)\n",
    "    return data\n",
    "\n",
    "dataset_path = 'hotpotQA_halu.json'\n",
    "qa_data = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "VuKoN0obY4MY",
   "metadata": {
    "id": "VuKoN0obY4MY"
   },
   "outputs": [],
   "source": [
    "# self-check consistency\n",
    "def sac2_score(question, target_answer, model, num_samples):\n",
    "\n",
    "    # llm evaluation\n",
    "    llm_evaluate = Evaluate(model=model)\n",
    "    scc = SemanticConsistnecyCheck(model=model)\n",
    "    # fast self-evaluation\n",
    "    self_responses = llm_evaluate.self_evaluate_api(self_question = question, temperature = 1.0, self_num = num_samples)\n",
    "    # fast consistency checker\n",
    "    consistency_res = scc.score_scc_api(question, target_answer, candidate_answers = self_responses, temperature = 0.0)\n",
    "\n",
    "    return consistency_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fA0Xn4FZVng",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fA0Xn4FZVng",
    "outputId": "d0cde700-3943-4d11-c0f2-c89e867ea784"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:32<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per query 3.041416354179382\n",
      "AUROC score 0.7432000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:59<00:00,  5.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per query 5.992955040931702\n",
      "AUROC score 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:16<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per query 3.9344684362411497\n",
      "AUROC score 0.7464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:46<00:00,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per query 4.532747850418091\n",
      "AUROC score 0.7456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# AUROC score for self-check\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "n_data = 50\n",
    "model = 'gpt-3.5-turbo'\n",
    "num_samples_list = [3,5,10,15]\n",
    "\n",
    "for num_samples in num_samples_list:\n",
    "\n",
    "    t0 = time.time()\n",
    "    halu_score_all = []\n",
    "    filename = 'halu_fastsac3_' + str(n_data) + '_' + str(num_samples) + '.txt'\n",
    "\n",
    "    for i in tqdm(range(n_data)):\n",
    "        if i <= n_data // 2:\n",
    "            target_answer = qa_data[i]['hallucinated_answer']\n",
    "        else:\n",
    "            target_answer = qa_data[i]['right_answer']\n",
    "\n",
    "        question = qa_data[i]['question']\n",
    "        sc2_score = sac2_score(question, target_answer, model, num_samples)\n",
    "        halu_score_all.append(sc2_score)\n",
    "\n",
    "    # auroc\n",
    "    print('Time per query', (time.time()-t0)/n_data)\n",
    "    true_label = [1]*(n_data // 2) + [0] * (n_data // 2)\n",
    "    roc_auc = roc_auc_score(true_label,halu_score_all)\n",
    "    print('AUROC score', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8PfBgO6hcXXJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PfBgO6hcXXJ",
    "outputId": "e45f3690-fe63-456d-821e-33a2948c549a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 50/50 [02:03<00:00,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per query: 2.47478000164032\n",
      "AUROC score: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# AUROC score for self-consistency without num_samples (50 data points)\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# self consistency\n",
    "def hallucination_score(question, target_answer, model):\n",
    "\n",
    "    # llm evaluation\n",
    "    llm_evaluate = Evaluate(model=model)\n",
    "    scc = SemanticConsistnecyCheck(model=model)\n",
    "    # fast self-evaluation\n",
    "    self_responses = llm_evaluate.self_evaluate_api(self_question = question, temperature = 1.0, self_num = 10)\n",
    "    # fast consistency checker\n",
    "    consistency_res = scc.score_scc_api(question, target_answer, candidate_answers = self_responses, temperature = 0.0)\n",
    "\n",
    "    return consistency_res[0]\n",
    "\n",
    "n_data = 50\n",
    "model = 'gpt-3.5-turbo'\n",
    "halu_score_all = []\n",
    "filename = 'halu_sac3_' + str(n_data) + '.txt'\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(n_data)):\n",
    "    if i < n_data // 2:\n",
    "        target_answer = qa_data[i]['hallucinated_answer']\n",
    "    else:\n",
    "        target_answer = qa_data[i]['right_answer']\n",
    "    question = qa_data[i]['question']\n",
    "\n",
    "    halu_score = hallucination_score(question, target_answer, model)\n",
    "    halu_score_all.append(halu_score)\n",
    "\n",
    "# Calculate and print the AUROC\n",
    "print('Time per query:', (time.time()-t0)/n_data)\n",
    "true_label = [1]*(n_data // 2) + [0] * (n_data // 2)\n",
    "roc_auc = roc_auc_score(true_label, halu_score_all)\n",
    "print(f'AUROC score: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foUCYGS3chTJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "foUCYGS3chTJ",
    "outputId": "3d99180b-3788-4b13-f789-679fd3f0e5ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:18<00:00,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per query: 4.381981918811798\n",
      "AUROC score: 0.7424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# AUROC score for self-consistency without num_samples (100 data points)\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# self consistency\n",
    "def hallucination_score(question, target_answer, model):\n",
    "\n",
    "    # llm evaluation\n",
    "    llm_evaluate = Evaluate(model=model)\n",
    "    scc = SemanticConsistnecyCheck(model=model)\n",
    "    # fast self-evaluation\n",
    "    fast_self_responses = llm_evaluate.self_evaluate_api(self_question = question, temperature = 1.0, self_num = 10)\n",
    "    # fast consistency checker\n",
    "    fast_consistency_res = scc.score_scc_api(question, target_answer, candidate_answers = fast_self_responses, temperature = 0.0)\n",
    "\n",
    "    return fast_consistency_res[0]\n",
    "\n",
    "n_data = 100\n",
    "model = 'gpt-3.5-turbo'\n",
    "halu_score_all = []\n",
    "filename = 'halu_sac3_' + str(n_data) + '.txt'\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(n_data)):\n",
    "    if i < n_data // 2:\n",
    "        target_answer = qa_data[i]['hallucinated_answer']\n",
    "    else:\n",
    "        target_answer = qa_data[i]['right_answer']\n",
    "    question = qa_data[i]['question']\n",
    "\n",
    "    halu_score = hallucination_score(question, target_answer, model)\n",
    "    halu_score_all.append(halu_score)\n",
    "\n",
    "# Calculate and print the AUROC\n",
    "print('Time per query:', (time.time()-t0)/n_data)\n",
    "true_label = [1]*(n_data // 2) + [0] * (n_data // 2)\n",
    "roc_auc = roc_auc_score(true_label, halu_score_all)\n",
    "print(f'AUROC score: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FitTUVHkL0WC",
   "metadata": {
    "id": "FitTUVHkL0WC"
   },
   "outputs": [],
   "source": [
    "def sac3_score(question, target_answer, model, num_samples):\n",
    "\n",
    "    # Initialize instances of Evaluate and SemanticConsistnecyCheck classes\n",
    "    llm_evaluate = Evaluate(model='gpt-3.5-turbo')\n",
    "    scc = SemanticConsistnecyCheck(model='gpt-3.5-turbo')\n",
    "\n",
    "    # question pertubation\n",
    "    gen_question = paraphrase(question, number = 10, model = 'gpt-3.5-turbo', temperature=0.0)\n",
    "\n",
    "    # evaluation\n",
    "    self_responses = llm_evaluate.self_evaluate_api(self_question=question, temperature=1.0, self_num=num_samples)\n",
    "    perb_responses = llm_evaluate.perb_evaluate_api(perb_questions = gen_question, temperature=0.0)\n",
    "\n",
    "    # consistency checker\n",
    "    consistency_res = scc.score_scc_api(question, target_answer, candidate_answers=perb_responses, temperature=0.0)\n",
    "\n",
    "    return consistency_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nGJVmfzKIQ4N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nGJVmfzKIQ4N",
    "outputId": "975c43a8-a977-401e-c40c-d29ae2be2369"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:00<00:00,  8.41s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per query 8.40940809249878\n",
      "AUROC score 0.8016\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:18<00:00,  8.76s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per query 8.76126256942749\n",
      "AUROC score 0.8168\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:15<00:00,  8.71s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per query 8.711775751113892\n",
      "AUROC score 0.7951999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [08:07<00:00,  9.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per query 9.754176802635193\n",
      "AUROC score 0.8168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# AUROC score for cross-check\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "n_data = 50\n",
    "model = 'gpt-3.5-turbo'\n",
    "num_samples_list = [3,5,10,15]\n",
    "\n",
    "for num_samples in num_samples_list:\n",
    "\n",
    "    t0 = time.time()\n",
    "    halu_score_all = []\n",
    "    filename = 'halu_fastsac3_' + str(n_data) + '_' + str(num_samples) + '.txt'\n",
    "\n",
    "    for i in tqdm(range(n_data)):\n",
    "        if i <= n_data // 2:\n",
    "            target_answer = qa_data[i]['hallucinated_answer']\n",
    "        else:\n",
    "            target_answer = qa_data[i]['right_answer']\n",
    "\n",
    "        question = qa_data[i]['question']\n",
    "        sc3_q_score = sac3_score(question, target_answer, model, num_samples)\n",
    "        halu_score_all.append(sc3_q_score)\n",
    "\n",
    "    # auroc\n",
    "    print('Time per query', (time.time()-t0)/n_data)\n",
    "    true_label = [1]*(n_data // 2) + [0] * (n_data // 2)\n",
    "    roc_auc = roc_auc_score(true_label,halu_score_all)\n",
    "    print('AUROC score', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fl200ByuIYQ3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fl200ByuIYQ3",
    "outputId": "f3c4f5c4-9988-460c-d964-2fe584a50843"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:52<00:00,  9.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per query: 9.441791810989379\n",
      "AUROC score: 0.8128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# AUROC score of cross-consistency without num_samples\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def sac3_score(question, target_answer, model):\n",
    "\n",
    "    # Initialize instances of Evaluate and SemanticConsistnecyCheck classes\n",
    "    llm_evaluate = Evaluate(model='gpt-3.5-turbo')\n",
    "    scc = SemanticConsistnecyCheck(model='gpt-3.5-turbo')\n",
    "\n",
    "    # question pertubation\n",
    "    gen_question = paraphrase(question, number = 10, model = 'gpt-3.5-turbo', temperature=0.0)\n",
    "\n",
    "    # evaluation\n",
    "    self_responses = llm_evaluate.self_evaluate_api(self_question=question, temperature=1.0, self_num=1)\n",
    "    perb_responses = llm_evaluate.perb_evaluate_api(perb_questions = gen_question, temperature=0.0)\n",
    "\n",
    "    # consistency checker\n",
    "    consistency_res = scc.score_scc_api(question, target_answer, candidate_answers=perb_responses, temperature=0.0)\n",
    "\n",
    "    return consistency_res[0]\n",
    "\n",
    "n_data = 50\n",
    "model = 'gpt-3.5-turbo'\n",
    "halu_score_all = []\n",
    "filename = 'halu_sac3_' + str(n_data) + '.txt'\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(n_data)):\n",
    "    if i < n_data // 2:\n",
    "        target_answer = qa_data[i]['hallucinated_answer']\n",
    "    else:\n",
    "        target_answer = qa_data[i]['right_answer']\n",
    "    question = qa_data[i]['question']\n",
    "\n",
    "    sac3_q_score = sac3_score(question, target_answer, model)\n",
    "    halu_score_all.append(sac3_q_score)\n",
    "\n",
    "# Calculate and print the AUROC\n",
    "print('Time per query:', (time.time()-t0)/n_data)\n",
    "true_label = [1]*(n_data // 2) + [0] * (n_data // 2)\n",
    "roc_auc = roc_auc_score(true_label, halu_score_all)\n",
    "print(f'AUROC score: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ByaZ2vl1qPr1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByaZ2vl1qPr1",
    "outputId": "09892724-3eaa-4440-de45-96229aba526e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [12:52<00:00,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per query: 7.727928912639618\n",
      "AUROC score: 0.7527999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# AUROC score of cross-consistency without num_samples\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def sac3_score(question, target_answer, model):\n",
    "\n",
    "    # Initialize instances of Evaluate and SemanticConsistnecyCheck classes\n",
    "    llm_evaluate = Evaluate(model='gpt-3.5-turbo')\n",
    "    scc = SemanticConsistnecyCheck(model='gpt-3.5-turbo')\n",
    "\n",
    "    # question pertubation\n",
    "    gen_question = paraphrase(question, number = 10, model = 'gpt-3.5-turbo', temperature=0.0)\n",
    "\n",
    "    # evaluation\n",
    "    self_responses = llm_evaluate.self_evaluate_api(self_question=question, temperature=1.0, self_num=3)\n",
    "    perb_responses = llm_evaluate.perb_evaluate_api(perb_questions = gen_question, temperature=0.0)\n",
    "\n",
    "    # consistency checker\n",
    "    consistency_res = scc.score_scc_api(question, target_answer, candidate_answers=perb_responses, temperature=0.0)\n",
    "\n",
    "    return consistency_res[0]\n",
    "\n",
    "n_data = 100\n",
    "model = 'gpt-3.5-turbo'\n",
    "halu_score_all = []\n",
    "filename = 'halu_sac3_' + str(n_data) + '.txt'\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in tqdm(range(n_data)):\n",
    "    if i < n_data // 2:\n",
    "        target_answer = qa_data[i]['hallucinated_answer']\n",
    "    else:\n",
    "        target_answer = qa_data[i]['right_answer']\n",
    "    question = qa_data[i]['question']\n",
    "\n",
    "    sac3_q_score = sac3_score(question, target_answer, model)\n",
    "    halu_score_all.append(sac3_q_score)\n",
    "\n",
    "# Calculate and print the AUROC\n",
    "print('Time per query:', (time.time()-t0)/n_data)\n",
    "true_label = [1]*(n_data // 2) + [0] * (n_data // 2)\n",
    "roc_auc = roc_auc_score(true_label, halu_score_all)\n",
    "print(f'AUROC score: {roc_auc}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
