{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "889caca0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "889caca0",
        "outputId": "841d50d1-aac3-47e1-8c44-f46f45bbb86a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28.0\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n",
            "Collecting peft\n",
            "  Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.2.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.40.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.2)\n",
            "Collecting accelerate>=0.21.0 (from peft)\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate, peft\n",
            "Successfully installed accelerate-0.29.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 peft-0.10.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28.0\n",
        "!pip install peft\n",
        "!pip install transformers\n",
        "!pip install numpy\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-nEuM2XFHdSn",
      "metadata": {
        "id": "-nEuM2XFHdSn"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "openai_api_key = getpass.getpass('Enter your OpenAI API key:')\n",
        "%env OPENAI_API_KEY=$openai_api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EqqvsCOjWxyk",
      "metadata": {
        "id": "EqqvsCOjWxyk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "f3849757-1415-40dd-86c8-b77ee1cd07cc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'openai'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9eb2adff7186>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# LLMs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# LLMs\n",
        "import openai\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Initialize OpenAI API\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "def call_openai_model(prompt, model, temperature):\n",
        "    response = None\n",
        "    while response is None:\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt},\n",
        "                ],\n",
        "                temperature = temperature\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            if 'is greater than the maximum' in str(e):\n",
        "                raise BatchSizeException()\n",
        "            print(e)\n",
        "            print('Retrying...')\n",
        "            time.sleep(2)\n",
        "        try:\n",
        "            output = response.choices[0].message.content\n",
        "        except Exception:\n",
        "            output = 'do not have reponse from chatgpt'\n",
        "    return output\n",
        "\n",
        "\n",
        "def call_guanaco_33b(prompt, max_new_tokens):\n",
        "    # 16 float\n",
        "    model_name = \"huggyllama/llama-30b\"\n",
        "    adapters_name = 'timdettmers/guanaco-33b'\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        # offload_folder=\"/home/ec2-user/SageMaker/hf_cache\",\n",
        "        max_memory= {i: '16384MB' for i in range(torch.cuda.device_count())}, # V100 16GB\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(model, adapters_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # prompt\n",
        "    formatted_prompt = (\n",
        "        f\"A chat between a curious human and an artificial intelligence assistant.\"\n",
        "        f\"The assistant gives helpful, concise, and polite answers to the user's questions.\\n\"\n",
        "        f\"### Human: {prompt} ### Assistant:\"\n",
        "    )\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "    outputs = model.generate(inputs=inputs.input_ids, max_new_tokens=max_new_tokens)\n",
        "    res = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    res_sp = res.split('###')\n",
        "    output = res_sp[1] + res_sp[2]\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def call_falcon_7b(prompt, max_new_tokens):\n",
        "    # 16 float\n",
        "    model = \"tiiuae/falcon-7b-instruct\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    pipeline = transformers.pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    sequences = pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    for seq in sequences:\n",
        "        res = seq['generated_text']\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R1u4IOGjYD7o",
      "metadata": {
        "id": "R1u4IOGjYD7o"
      },
      "outputs": [],
      "source": [
        "# paraphraser\n",
        "def paraphrase(question, number, model, temperature):\n",
        "    '''\n",
        "    Inputs:\n",
        "    quesiton - original user query\n",
        "    number - how many perturbed questions\n",
        "    model - GPTs or open-sourced models\n",
        "    temperature - typically we use 0 here\n",
        "\n",
        "    Output:\n",
        "    perb_questions - perturbed questions that are semantically equivalent to the question\n",
        "    '''\n",
        "\n",
        "    perb_questions = []\n",
        "    prompt_temp = f'For question Q, provide {number} semantically equivalent questions.'\n",
        "    prompt = prompt_temp + '\\nQ:' + question\n",
        "\n",
        "    res = call_openai_model(prompt, model, temperature) # openai model call\n",
        "    res_split = res.split('\\n')\n",
        "    for i in range(len(res_split)):\n",
        "        perb_questions.append(res_split[i])\n",
        "\n",
        "    return perb_questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6EzG0rWbLmXd",
      "metadata": {
        "id": "6EzG0rWbLmXd"
      },
      "outputs": [],
      "source": [
        "# Consistency_Checker\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import concurrent.futures\n",
        "\n",
        "class SemanticConsistnecyCheck:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.prompt_temp = \"\"\"\n",
        "        Are the following two Question-Answer(QA) pairs semantically equivalent?\n",
        "        Provide your best guess and the probability that it is correct (0.0 to 1.0).\n",
        "        Given ONLY the guess (Yes or No) and probability, no other words or explanation.\n",
        "        For example:\n",
        "        Guess: <most likely guess, as short as possible; not a complete sentence, just the guess!>\n",
        "        Probability: <the probability between 0.0 and 1.0 that your guess is correct, without any extra commentary whatsoever;\n",
        "        just the probability!>\n",
        "        \"\"\"\n",
        "\n",
        "    def openai_api_parallel(self, prompt, temperature):\n",
        "        res = call_openai_model(prompt, self.model, temperature) # openai model call\n",
        "        return res\n",
        "\n",
        "    def score_scc_api(self, question, target_answer, candidate_answers, temperature):\n",
        "\n",
        "        if target_answer is None:\n",
        "            raise ValueError(\"Target answer cannot be None. \")\n",
        "\n",
        "        sc_output = []\n",
        "        target_pair = 'Q:' + question + '\\nA:' + target_answer\n",
        "        num_candidate_answer = len(candidate_answers)\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=num_candidate_answer+2) as executor:\n",
        "            all_res = []\n",
        "            for i in range(num_candidate_answer):\n",
        "                candidate_pair = 'Q:' + question + '\\nA:' + candidate_answers[i]\n",
        "                prompt = self.prompt_temp + '\\nThe first QA pair is:\\n' + target_pair + '\\nThe second QA pair is:\\n' + candidate_pair\n",
        "                output = executor.submit(self.openai_api_parallel, prompt, temperature)\n",
        "                all_res.append(output)\n",
        "\n",
        "            for temp in concurrent.futures.as_completed(all_res):\n",
        "                res = temp.result()\n",
        "                guess = res.split(':')[1].split('\\n')[0].strip()\n",
        "                # print(res, guess)\n",
        "                value = 0 if guess == 'Yes' else 1\n",
        "                # print('value',value)\n",
        "                sc_output.append(value)\n",
        "\n",
        "        score = sum(sc_output)/num_candidate_answer\n",
        "        return score, sc_output\n",
        "\n",
        "\n",
        "\n",
        "    def score_scc(self, question, target_answer, candidate_answers, temperature):\n",
        "        '''\n",
        "        Inputs:\n",
        "        question - original user query\n",
        "        target_answer - generated response given the original question (temp=0) if not provided by user\n",
        "        candidate_answers - generated responses given the question (original + perturbed)\n",
        "        temperature - [0,1] for LLM randomness\n",
        "\n",
        "        Outputs:\n",
        "        score - inconsistency score (hallucination metric)\n",
        "        sc_output - specific score for each candidate answers compared with the target answer\n",
        "        '''\n",
        "\n",
        "        if target_answer is None:\n",
        "            raise ValueError(\"Target answer cannot be None. \")\n",
        "\n",
        "        sc_output = []\n",
        "        target_pair = 'Q:' + question + '\\nA:' + target_answer\n",
        "        num_candidate_answer = len(candidate_answers)\n",
        "        for i in range(num_candidate_answer):\n",
        "            candidate_pair = 'Q:' + question + '\\nA:' + candidate_answers[i]\n",
        "            prompt = self.prompt_temp + '\\nThe first QA pair is:\\n' + target_pair + '\\nThe second QA pair is:\\n' + candidate_pair\n",
        "            res = call_openai_model(prompt, self.model, temperature) # openai model call\n",
        "            # res = call_guanaco_33b(prompt, max_new_tokens=200)  # guanaco_33b model call\n",
        "            # res = call_falcon_7b(prompt, max_new_tokens = 200) # falcon_7b model call\n",
        "            guess = res.split(':')[1].split('\\n')[0].strip()\n",
        "            # print(res, guess)\n",
        "            value = 0 if guess == 'Yes' else 1\n",
        "            # print('value',value)\n",
        "            sc_output.append(value)\n",
        "\n",
        "        score = sum(sc_output)/num_candidate_answer\n",
        "        return score, sc_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hNM8NNHgYa-U",
      "metadata": {
        "id": "hNM8NNHgYa-U"
      },
      "outputs": [],
      "source": [
        "# evaluator\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import concurrent.futures\n",
        "\n",
        "class Evaluate:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.prompt_temp = 'Please answer the following question:\\n'\n",
        "\n",
        "    def openai_api_parallel(self, prompt, temperature):\n",
        "        res = call_openai_model(prompt, self.model, temperature) # openai model call\n",
        "        return res\n",
        "\n",
        "#     def self_evaluate_api(self, self_question, temperature, self_num):\n",
        "\n",
        "#         prompt = self.prompt_temp + '\\nQ:' + self_question\n",
        "#         self_responses = []\n",
        "#         with ThreadPoolExecutor(max_workers=self_num) as executor:\n",
        "#             outputs = executor.map(self.openai_api_parallel, prompt, temperature)\n",
        "#             for res in outputs:\n",
        "#                 self_responses.append(res)\n",
        "\n",
        "#         return self_responses\n",
        "\n",
        "    def self_evaluate_api(self, self_question, temperature, self_num):\n",
        "\n",
        "        prompt = self.prompt_temp + self_question\n",
        "        self_responses = []\n",
        "        with ThreadPoolExecutor(max_workers=self_num) as executor:\n",
        "            futures = [executor.submit(self.openai_api_parallel, prompt, temperature) for _ in range(self_num)]\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                self_responses.append(future.result())\n",
        "\n",
        "        return self_responses\n",
        "\n",
        "\n",
        "\n",
        "    def self_evaluate(self, self_question, temperature, self_num):\n",
        "        '''\n",
        "        Inputs:\n",
        "        self_question - original user query\n",
        "        temperature - [0,1] for LLM randomness\n",
        "        self_num - how many generated responses given this question\n",
        "\n",
        "        Outputs:\n",
        "        self_responses - generated responses given this question with different temperatures\n",
        "        '''\n",
        "\n",
        "        self_responses = []\n",
        "        prompt = self.prompt_temp + '\\nQ:' + self_question\n",
        "\n",
        "        for i in range(self_num):\n",
        "            # llm model: GPTs, open-source models (falcon, guanaco)\n",
        "            if self.model in ['gpt-3.5-turbo','gpt-4']:\n",
        "                res = call_openai_model(prompt, self.model, temperature) # openai model call\n",
        "            elif self.model == 'guanaco-33b':\n",
        "                res = call_guanaco_33b(prompt, max_new_tokens = 200)\n",
        "            elif self.model == 'falcon-7b':\n",
        "                res = call_falcon_7b(prompt, max_new_tokens = 200)\n",
        "            # other open-sourced llms\n",
        "            self_responses.append(res)\n",
        "\n",
        "        # thread\n",
        "\n",
        "        return self_responses\n",
        "\n",
        "\n",
        "    def perb_evaluate_api(self, perb_questions, temperature):\n",
        "        '''\n",
        "        Inputs:\n",
        "        perb_questions - perturbed questions that are semantically equivalent to the original question\n",
        "        temperature - [0,1] for LLM randomness\n",
        "\n",
        "        Outputs:\n",
        "        perb_responses - generated responses given the perturbed questions\n",
        "        '''\n",
        "\n",
        "        perb_responses = []\n",
        "        with ThreadPoolExecutor(max_workers=len(perb_questions)) as executor:\n",
        "            future_to_pq = {\n",
        "                executor.submit(self.openai_api_parallel, self.prompt_temp + perb_question, temperature): perb_question\n",
        "                for perb_question in perb_questions\n",
        "            }\n",
        "\n",
        "            for future in concurrent.futures.as_completed(future_to_pq):\n",
        "                perb_question = future_to_pq[future]\n",
        "                try:\n",
        "                    perb_responses.append(future.result())\n",
        "                except Exception as exc:\n",
        "                    print('%r generated an exception: %s' % (perb_question, exc))\n",
        "\n",
        "        return perb_responses\n",
        "\n",
        "\n",
        "\n",
        "    def perb_evaluate(self, perb_questions, temperature):\n",
        "        '''\n",
        "        Inputs:\n",
        "        perb_questions - perturbed questions that are semantically equivalent to the original question\n",
        "        temperature - [0,1] for LLM randomness\n",
        "\n",
        "        Outputs:\n",
        "        perb_responses - generated responses given the perturbed questions\n",
        "        '''\n",
        "\n",
        "        perb_responses = []\n",
        "        for i in range(len(perb_questions)):\n",
        "            prompt = self.prompt_temp + '\\nQ:' + perb_questions[i]\n",
        "            # llm model: GPTs, open-source models (falcon, guanaco)\n",
        "            if self.model in ['gpt-3.5-turbo','gpt-4']:\n",
        "                res = call_openai_model(prompt, self.model, temperature) # openai model call\n",
        "            elif self.model == 'guanaco-33b':\n",
        "                res = call_guanaco_33b(prompt, max_new_tokens = 200)\n",
        "            elif self.model == 'falcon-7b':\n",
        "                res = call_falcon_7b(prompt, max_new_tokens = 200)\n",
        "            # other open-sourced llms\n",
        "            perb_responses.append(res)\n",
        "\n",
        "        return perb_responses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input information\n",
        "question = 'is pi smaller than 3.2?'\n",
        "target_answer = \"Yes\"\n",
        "\n",
        "# question pertubation\n",
        "gen_question = paraphrase(question, number = 2, model = 'gpt-3.5-turbo', temperature = 1.0)\n",
        "\n",
        "# llm evaluation\n",
        "llm_evaluate = Evaluate(model='gpt-3.5-turbo')\n",
        "self_responses = llm_evaluate.self_evaluate_api(self_question = question, temperature = 1.0, self_num = 2)\n",
        "perb_responses = llm_evaluate.perb_evaluate_api(perb_questions = gen_question, temperature = 0.0)\n",
        "\n",
        "print('Original question', question)\n",
        "print('self_responses', self_responses)\n",
        "print('perb_responses', perb_responses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZNM3cDDF03z",
        "outputId": "841a31f5-7bd9-4fae-87b6-a1fffc376c9b"
      },
      "id": "sZNM3cDDF03z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original question is pi smaller than 3.2?\n",
            "self_responses ['No, π (pi) is not smaller than 3.2. \\n\\nπ is approximately equal to 3.14159, which is smaller than 3.2.', 'No, pi (π) is not smaller than 3.2. Pi is approximately 3.14159... and is larger than 3.2.']\n",
            "perb_responses ['Yes, pi (π) is less than 3.2. The value of pi is approximately 3.14159.', '1. \"Is the value of pi less than 3.2?\"\\n2. \"Does pi have a value that is less than 3.2?\"', 'Yes, the value of pi (π) is approximately 3.14159, which is less than 3.2.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# llm evaluation\n",
        "llm_evaluate = Evaluate(model='gpt-3.5-turbo')\n",
        "self_responses = llm_evaluate.self_evaluate(self_question = question, temperature = 1.0, self_num = 3)\n",
        "perb_responses = llm_evaluate.perb_evaluate(perb_questions = gen_question, temperature=0.0)\n",
        "\n",
        "# consistency check\n",
        "scc = SemanticConsistnecyCheck(model='gpt-3.5-turbo')\n",
        "\n",
        "sc2_score, sc2_vote = scc.score_scc(question, target_answer, candidate_answers = self_responses, temperature = 0.0)\n",
        "print(sc2_score, sc2_vote)\n",
        "\n",
        "sac3_q_score, sac3_q_vote = scc.score_scc(question, target_answer, candidate_answers = perb_responses, temperature = 0.0)\n",
        "print(sac3_q_score, sac3_q_vote)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNd6fizuPfmY",
        "outputId": "eac96165-a15b-420c-b675-1699f619c5e2"
      },
      "id": "bNd6fizuPfmY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0 [0, 0, 0]\n",
            "0.3333333333333333 [0, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PPnpjcoXoirr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "PPnpjcoXoirr",
        "outputId": "7a7bdb27-8745-4aad-eb8c-b0018cc672f6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3a193872-1710-4194-bf8d-ec2f0d07e998\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3a193872-1710-4194-bf8d-ec2f0d07e998\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving hotpotQA_halu.json to hotpotQA_halu.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lGhbUuZzpHx5",
      "metadata": {
        "id": "lGhbUuZzpHx5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def load_dataset(filepath):\n",
        "    data = []\n",
        "    with open(filepath, 'r') as file:\n",
        "        for line in file:\n",
        "            json_obj = json.loads(line)\n",
        "            data.append(json_obj)\n",
        "    return data\n",
        "\n",
        "dataset_path = 'hotpotQA_halu.json'\n",
        "qa_data = load_dataset(dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VuKoN0obY4MY",
      "metadata": {
        "id": "VuKoN0obY4MY"
      },
      "outputs": [],
      "source": [
        "# self-check consistency\n",
        "def sac2_score(question, target_answer, model, num_samples):\n",
        "\n",
        "    # llm evaluation\n",
        "    llm_evaluate = Evaluate(model=model)\n",
        "    scc = SemanticConsistnecyCheck(model=model)\n",
        "    # self-evaluation\n",
        "    self_responses = llm_evaluate.self_evaluate_api(self_question = question, temperature = 1.0, self_num = num_samples)\n",
        "    # consistency checker\n",
        "    consistency_res = scc.score_scc_api(question, target_answer, candidate_answers = self_responses, temperature = 0.0)\n",
        "\n",
        "    return consistency_res[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fA0Xn4FZVng",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fA0Xn4FZVng",
        "outputId": "822a32e9-b217-4b9c-e6eb-92a5148433ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [02:01<00:00,  2.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time per query: 2.4351020193099977\n",
            "AUROC score for 3 samples: 0.744\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [02:02<00:00,  2.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time per query: 2.4488013648986815\n",
            "AUROC score for 5 samples: 0.7536\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [02:37<00:00,  3.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time per query: 3.159844436645508\n",
            "AUROC score for 10 samples: 0.7408\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [03:09<00:00,  3.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time per query: 3.7828142642974854\n",
            "AUROC score for 15 samples: 0.7824000000000001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# AUROC score for self-check\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "n_data = 50\n",
        "model = 'gpt-3.5-turbo'\n",
        "num_samples_list = [3, 5, 10, 15]\n",
        "\n",
        "# Randomly sample 50 questions from qa_data\n",
        "sampled_qa_data = random.sample(qa_data, n_data)\n",
        "\n",
        "for num_samples in num_samples_list:\n",
        "    t0 = time.time()\n",
        "    halu_score_all = []\n",
        "\n",
        "    for i in tqdm(range(n_data)):\n",
        "        if i < n_data // 2:\n",
        "            target_answer = sampled_qa_data[i]['hallucinated_answer']\n",
        "        else:\n",
        "            target_answer = sampled_qa_data[i]['right_answer']\n",
        "\n",
        "        # Use the correct reference for question\n",
        "        question = sampled_qa_data[i]['question']\n",
        "\n",
        "        # Call sac2_score for each sampled question and target answer\n",
        "        sc2_score = sac2_score(question, target_answer, model, num_samples)\n",
        "\n",
        "        # Append the score to halu_score_all\n",
        "        halu_score_all.append(sc2_score)\n",
        "\n",
        "    # Calculate and print the AUROC\n",
        "    print('Time per query:', (time.time() - t0) / n_data)\n",
        "    true_label = [1] * (n_data // 2) + [0] * (n_data // 2)\n",
        "    roc_auc = roc_auc_score(true_label, halu_score_all)\n",
        "    print('AUROC score for', num_samples, 'samples:', roc_auc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8PfBgO6hcXXJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PfBgO6hcXXJ",
        "outputId": "752a9b13-25be-426a-c052-05f380b138cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [03:07<00:00,  3.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time per query: 3.7432323694229126\n",
            "AUROC score: 0.7568\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# AUROC score for self-consistency with 10 responses (50 data points)\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# self consistency\n",
        "def hallucination_score(question, target_answer, model):\n",
        "\n",
        "    # llm evaluation\n",
        "    llm_evaluate = Evaluate(model=model)\n",
        "    scc = SemanticConsistnecyCheck(model=model)\n",
        "    # fast self-evaluation\n",
        "    self_responses = llm_evaluate.self_evaluate_api(self_question = question, temperature = 1.0, self_num = 10)\n",
        "    # fast consistency checker\n",
        "    consistency_res = scc.score_scc_api(question, target_answer, candidate_answers = self_responses, temperature = 0.0)\n",
        "\n",
        "    return consistency_res[0]\n",
        "\n",
        "n_data = 50\n",
        "model = 'gpt-3.5-turbo'\n",
        "halu_score_all = []\n",
        "filename = 'halu_sac3_' + str(n_data) + '.txt'\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "# Randomly sample 50 questions from qa_data\n",
        "sampled_qa_data = random.sample(qa_data, n_data)\n",
        "\n",
        "for i in tqdm(range(n_data)):\n",
        "    if i < n_data // 2:\n",
        "        target_answer = sampled_qa_data[i]['hallucinated_answer']\n",
        "    else:\n",
        "        target_answer = sampled_qa_data[i]['right_answer']\n",
        "\n",
        "    question = sampled_qa_data[i]['question']\n",
        "\n",
        "    halu_score = hallucination_score(question, target_answer, model)\n",
        "    halu_score_all.append(halu_score)\n",
        "\n",
        "# Calculate and print the AUROC\n",
        "print('Time per query:', (time.time()-t0)/n_data)\n",
        "true_label = [1]*(n_data // 2) + [0] * (n_data // 2)\n",
        "roc_auc = roc_auc_score(true_label, halu_score_all)\n",
        "print('AUROC score:', roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "foUCYGS3chTJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foUCYGS3chTJ",
        "outputId": "fb4f6b79-5352-4c9f-c3f4-9105a142e3f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [04:35<00:00,  2.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time per query: 2.758979046344757\n",
            "AUROC score: 0.754\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# AUROC score for self-consistency with 10 responses (100 data points)\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# self consistency\n",
        "def hallucination_score(question, target_answer, model):\n",
        "\n",
        "    # llm evaluation\n",
        "    llm_evaluate = Evaluate(model=model)\n",
        "    scc = SemanticConsistnecyCheck(model=model)\n",
        "    # fast self-evaluation\n",
        "    fast_self_responses = llm_evaluate.self_evaluate_api(self_question = question, temperature = 1.0, self_num = 10)\n",
        "    # fast consistency checker\n",
        "    fast_consistency_res = scc.score_scc_api(question, target_answer, candidate_answers = fast_self_responses, temperature = 0.0)\n",
        "\n",
        "    return fast_consistency_res[0]\n",
        "\n",
        "n_data = 100\n",
        "model = 'gpt-3.5-turbo'\n",
        "halu_score_all = []\n",
        "\n",
        "# Randomly sample 100 questions from qa_data\n",
        "sampled_qa_data = random.sample(qa_data, n_data)\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "for i in tqdm(range(n_data)):\n",
        "    if i < n_data // 2:\n",
        "        target_answer = sampled_qa_data[i]['hallucinated_answer']\n",
        "    else:\n",
        "        target_answer = sampled_qa_data[i]['right_answer']\n",
        "\n",
        "    question = sampled_qa_data[i]['question']\n",
        "\n",
        "    halu_score = hallucination_score(question, target_answer, model)\n",
        "    halu_score_all.append(halu_score)\n",
        "\n",
        "# Calculate and print the AUROC\n",
        "print('Time per query:', (time.time()-t0)/n_data)\n",
        "true_label = [1]*(n_data // 2) + [0] * (n_data // 2)\n",
        "roc_auc = roc_auc_score(true_label, halu_score_all)\n",
        "print(f'AUROC score: {roc_auc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FitTUVHkL0WC",
      "metadata": {
        "id": "FitTUVHkL0WC"
      },
      "outputs": [],
      "source": [
        "def sac3_score(question, target_answer, model, num_samples):\n",
        "\n",
        "    # Initialize instances of Evaluate and SemanticConsistnecyCheck classes\n",
        "    llm_evaluate = Evaluate(model='gpt-3.5-turbo')\n",
        "    scc = SemanticConsistnecyCheck(model='gpt-3.5-turbo')\n",
        "\n",
        "    # question pertubation\n",
        "    gen_question = paraphrase(question, number = 10, model = 'gpt-3.5-turbo', temperature=0.0)\n",
        "\n",
        "    # evaluation\n",
        "    self_responses = llm_evaluate.self_evaluate_api(self_question=question, temperature=1.0, self_num=num_samples)\n",
        "    perb_responses = llm_evaluate.perb_evaluate_api(perb_questions = gen_question, temperature=0.0)\n",
        "\n",
        "    # consistency checker\n",
        "    consistency_res = scc.score_scc_api(question, target_answer, candidate_answers=perb_responses, temperature=0.0)\n",
        "\n",
        "    return consistency_res[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nGJVmfzKIQ4N",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGJVmfzKIQ4N",
        "outputId": "b24b6206-cb8b-4327-866d-8d23a1990400"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [06:43<00:00,  8.08s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time per query 8.077125840187072\n",
            "AUROC score for 3 is: 0.7984\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [07:01<00:00,  8.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time per query 8.427486939430237\n",
            "AUROC score for 5 is: 0.8008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [07:05<00:00,  8.50s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time per query 8.504774408340454\n",
            "AUROC score for 10 is: 0.8432\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [07:43<00:00,  9.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time per query 9.275331087112427\n",
            "AUROC score for 15 is: 0.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# AUROC score for cross-check\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "n_data = 50\n",
        "model = 'gpt-3.5-turbo'\n",
        "num_samples_list = [3,5,10,15]\n",
        "\n",
        "# Randomly sample 50 questions from qa_data\n",
        "sampled_qa_data = random.sample(qa_data, n_data)\n",
        "\n",
        "for num_samples in num_samples_list:\n",
        "\n",
        "    t0 = time.time()\n",
        "    halu_score_all = []\n",
        "\n",
        "    for i in tqdm(range(n_data)):\n",
        "      if i < n_data // 2:\n",
        "        target_answer = sampled_qa_data[i]['hallucinated_answer']\n",
        "      else:\n",
        "        target_answer = sampled_qa_data[i]['right_answer']\n",
        "\n",
        "      question = sampled_qa_data[i]['question']\n",
        "\n",
        "      sac3_q_score = sac3_score(question, target_answer, model, num_samples)\n",
        "      halu_score_all.append(sac3_q_score)\n",
        "\n",
        "    # auroc\n",
        "    print('Time per query', (time.time()-t0)/n_data)\n",
        "    true_label = [1]*(n_data // 2) + [0] * (n_data // 2)\n",
        "    roc_auc = roc_auc_score(true_label,halu_score_all)\n",
        "    print('AUROC score for', num_samples, 'is:', roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3MaqhyL9vTDU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MaqhyL9vTDU",
        "outputId": "0c4acfec-0a15-42f2-984d-fff9ca268a43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [06:09<00:00,  7.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time per query 7.399755811691284\n",
            "AUROC score is: 0.8696\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# AUROC score for cross-check for 50 data points with random sampling\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def sac3_score(question, target_answer, model):\n",
        "\n",
        "    # Initialize instances of Evaluate and SemanticConsistnecyCheck classes\n",
        "    llm_evaluate = Evaluate(model=model)\n",
        "    scc = SemanticConsistnecyCheck(model=model)\n",
        "\n",
        "    # question pertubation\n",
        "    gen_question = paraphrase(question, number = 10, model = 'gpt-3.5-turbo', temperature=0.0)\n",
        "\n",
        "    # evaluation\n",
        "    self_responses = llm_evaluate.self_evaluate_api(self_question=question, temperature=1.0, self_num=1)\n",
        "    perb_responses = llm_evaluate.perb_evaluate_api(perb_questions = gen_question, temperature=0.0)\n",
        "\n",
        "    # consistency checker\n",
        "    consistency_res = scc.score_scc_api(question, target_answer, candidate_answers=perb_responses, temperature=0.0)\n",
        "\n",
        "    return consistency_res[0]\n",
        "\n",
        "n_data = 50\n",
        "model = 'gpt-3.5-turbo'\n",
        "# Randomly sample 50 questions from qa_data\n",
        "sampled_qa_data = random.sample(qa_data, n_data)\n",
        "\n",
        "t0 = time.time()\n",
        "halu_score_all = []\n",
        "\n",
        "for i in tqdm(range(n_data)):\n",
        "  if i < n_data // 2:\n",
        "      target_answer = sampled_qa_data[i]['hallucinated_answer']\n",
        "  else:\n",
        "      target_answer = sampled_qa_data[i]['right_answer']\n",
        "\n",
        "  question = sampled_qa_data[i]['question']\n",
        "\n",
        "  sac3_q_score = sac3_score(question, target_answer, model)\n",
        "  halu_score_all.append(sac3_q_score)\n",
        "\n",
        "# auroc\n",
        "print('Time per query', (time.time()-t0)/n_data)\n",
        "true_label = [1]*(n_data // 2) + [0] * (n_data // 2)\n",
        "roc_auc = roc_auc_score(true_label,halu_score_all)\n",
        "print('AUROC score is:', roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C7v5VLWahxag",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7v5VLWahxag",
        "outputId": "ec5a3578-a4f0-4e42-b67a-b42082789d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [10:44<00:00,  6.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time per query: 6.446413452625275\n",
            "AUROC score: 0.7795999999999998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# AUROC score of cross-consistency with 100 questions\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def sac3_score(question, target_answer, model):\n",
        "\n",
        "    # Initialize instances of Evaluate and SemanticConsistnecyCheck classes\n",
        "    llm_evaluate = Evaluate(model='gpt-3.5-turbo')\n",
        "    scc = SemanticConsistnecyCheck(model='gpt-3.5-turbo')\n",
        "\n",
        "    # question pertubation\n",
        "    gen_question = paraphrase(question, number = 10, model = 'gpt-3.5-turbo', temperature=0.0)\n",
        "\n",
        "    # evaluation\n",
        "    self_responses = llm_evaluate.self_evaluate_api(self_question=question, temperature=1.0, self_num=1)\n",
        "    perb_responses = llm_evaluate.perb_evaluate_api(perb_questions = gen_question, temperature=0.0)\n",
        "\n",
        "    # consistency checker\n",
        "    consistency_res = scc.score_scc_api(question, target_answer, candidate_answers=perb_responses, temperature=0.0)\n",
        "\n",
        "    return consistency_res[0]\n",
        "\n",
        "n_data = 100\n",
        "model = 'gpt-3.5-turbo'\n",
        "halu_score_all = []\n",
        "\n",
        "# Randomly sample 100 questions from qa_data\n",
        "sampled_qa_data = random.sample(qa_data, n_data)\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "for i in tqdm(range(n_data)):\n",
        "    if i < n_data // 2:\n",
        "        target_answer = sampled_qa_data[i]['hallucinated_answer']\n",
        "    else:\n",
        "        target_answer = sampled_qa_data[i]['right_answer']\n",
        "    question = sampled_qa_data[i]['question']\n",
        "\n",
        "    sac3_q_score = sac3_score(question, target_answer, model)\n",
        "    halu_score_all.append(sac3_q_score)\n",
        "\n",
        "# Calculate and print the AUROC\n",
        "print('Time per query:', (time.time()-t0)/n_data)\n",
        "true_label = [1]*(n_data // 2) + [0] * (n_data // 2)\n",
        "roc_auc = roc_auc_score(true_label, halu_score_all)\n",
        "print(f'AUROC score: {roc_auc}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}